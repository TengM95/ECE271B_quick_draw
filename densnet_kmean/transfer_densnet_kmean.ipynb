{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = '../labels.txt'\n",
    "f = open(label_dir,\"r\")\n",
    "# And for reading use\n",
    "classes = f.read()\n",
    "f.close()\n",
    "classes_list = classes.split('\\n')\n",
    "count = [int(i) for i in classes_list[::2]]\n",
    "max_count = max(count)\n",
    "weights = 1.0*max_count/np.array(count)\n",
    "class_weights = torch.FloatTensor(weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model = models.densenet169(pretrained=True)\n",
    "model.classifier = nn.Sequential(nn.Linear(1664, 359))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 10           # Number of full passes through the dataset\n",
    "batch_size = 32         # Number of samples in each minibatch\n",
    "learning_rate = 0.01  \n",
    "seed = np.random.seed(0) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.ToPILImage('L'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomRotation(60),\n",
    "        transforms.Resize([224,224],interpolation=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "model = model\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to modify\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "import quickdraw_dataloader as qd\n",
    "from quickdraw_dataloader import create_split_loaders\n",
    "root_dir = \"../data_subset/\"\n",
    "train_loader, val_loader, test_loader = create_split_loaders(root_dir,batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights).to(computing_device)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader,model,optimizer,criterion):\n",
    "    start = time.time()\n",
    "    sum_loss = 0.0\n",
    "    list_sum_loss = []\n",
    "    correct_overall = 0.0\n",
    "    minbatch_size = 0.0\n",
    "    num = 0\n",
    "    for mb_count, (val_images, val_labels) in enumerate(val_loader, 0):\n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            optimizer.zero_grad()      \n",
    "            val_images = torch.squeeze(torch.stack([val_images,val_images,val_images], dim=1, out=None))\n",
    "            val_images, val_labels = val_images.to(computing_device), val_labels.to(computing_device)\n",
    "            val_labels = val_labels.type(torch.cuda.FloatTensor)\n",
    "            val_labels = val_labels.long()\n",
    "            outputs = model(val_images)\n",
    "            \n",
    "            loss = criterion(outputs,val_labels)\n",
    "            sum_loss += loss\n",
    "            \n",
    "            output_np = outputs.cpu().detach().numpy()\n",
    "            label_np = val_labels.cpu().detach().numpy()\n",
    "            correct_prediction, minbatch_size = correctness(label_np, output_np)\n",
    "            correct_overall += correct_prediction\n",
    "            minbatch_size += minbatch_size\n",
    "    accuracy_vali = correct_overall/minbatch_size\n",
    "    print('validatation accuracy',accuracy_vali) \n",
    "    \n",
    "    print(\"validation time = \", time.time()-start)    \n",
    "    return 1.0*sum_loss/mb_count, accuracy_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness(labels, out):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs==labels), labels.size\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10,resume=False, direct=\"\"):\n",
    "    since = time.time()\n",
    "    total_loss = []\n",
    "    if resume == False:\n",
    "        avg_minibatch_loss = []\n",
    "        total_vali_loss = []\n",
    "        avg_training_accuracy = []\n",
    "        avg_validation_accuracy = []\n",
    "        epc_save = 0 # MAYBE\n",
    "    else: \n",
    "        print('Resume model: %s' % direct)\n",
    "        check_point = torch.load(direct)\n",
    "        model.load_state_dict(check_point['state_dict'])\n",
    "        optimizer.load_state_dict(check_point['optimizer'])\n",
    "        avg_minibatch_loss = list(np.load('avg_minibatch_loss.npy'))\n",
    "        total_vali_loss = list(np.load('total_vali_loss.npy'))\n",
    "        avg_training_accuracy = list(np.load('avg_training_accuracy.npy'))\n",
    "        avg_validation_accuracy = list(np.load('avg_validation_accuracy.npy'))\n",
    "        epc_save = check_point['epoch']\n",
    "        \n",
    "    tolerence = 3\n",
    "    i = 0 \n",
    "    for epoch in range(epc_save,num_epochs):\n",
    "        N = 100\n",
    "        M = 700\n",
    "        N_minibatch_loss = 0.0    \n",
    "        best_loss = 100\n",
    "        early_stop = 0\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        correct_overall = 0.0; \n",
    "        minbatch_size_overall = 0.0; \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            scheduler.step()\n",
    "            # Iterate over data.\n",
    "            minibatch_time = time.time()\n",
    "            for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs = torch.squeeze(torch.stack([inputs,inputs,inputs], dim=1, out=None))\n",
    "                inputs = inputs.to(computing_device)\n",
    "                labels = labels.to(computing_device)\n",
    "                labels = labels.long()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.long()\n",
    "                    loss = criterion(outputs,labels)\n",
    "                    #loss = criterion(torch.max(outputs,1)[1],torch.max(labels, 1)[1])\n",
    "                    N_minibatch_loss += loss\n",
    "                    # backward + optimize only if in training phase\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    output_np = outputs.cpu().detach().numpy()\n",
    "                    label_np = labels.cpu().detach().numpy()\n",
    "\n",
    "                    correct_prediction, minbatch_size = correctness(label_np, output_np)\n",
    "                    correct_overall += correct_prediction\n",
    "                    minbatch_size_overall += minbatch_size\n",
    "                # statistics\n",
    "                # training stats\n",
    "                if minibatch_count % N == 0 and minibatch_count!=0:    \n",
    "\n",
    "                    # Print the loss averaged over the last N mini-batches    \n",
    "                    N_minibatch_loss /= N\n",
    "                    print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                        (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "                    accuracy_train = correct_overall/minbatch_size_overall\n",
    "                    print('accuracy',accuracy_train)\n",
    "                    \n",
    "                    \n",
    "                    # Add the averaged loss over N minibatches and reset the counter\n",
    "                    avg_minibatch_loss.append(N_minibatch_loss)\n",
    "                    avg_minibatch_loss_1 = np.array(avg_minibatch_loss)\n",
    "                    np.save('avg_minibatch_loss', avg_minibatch_loss_1)\n",
    "                    avg_training_accuracy.append(accuracy_train)\n",
    "                    avg_training_accuracy_1 = np.array(avg_training_accuracy)\n",
    "                    np.save('avg_training_accuracy', avg_training_accuracy)\n",
    "                    \n",
    "                    N_minibatch_loss = 0.0\n",
    "                    correct_overall = 0.0\n",
    "                    minbatch_size_overall = 0.0\n",
    "                    #print('accuracy, precision, recall', accuracy, precision, recall)\n",
    "                    save_checkpoint({'epoch': epoch + 1,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict(),\n",
    "                                },\n",
    "                                filename='./checkpoint/'+'%d_model_epoch%d.pth' % (epoch , minibatch_count))\n",
    "                    print(N, \"minibatch_time\" , time.time()-minibatch_time)\n",
    "                    minibatch_time = time.time()\n",
    "                #Validation\n",
    "                if minibatch_count % M == 0 and minibatch_count!=0:\n",
    "                   \n",
    "                    v_loss, accuracy_vali = validate(val_loader,model,optimizer,criterion)\n",
    "                    print(v_loss.item())\n",
    "                    \n",
    "                    total_vali_loss.append(v_loss.item())\n",
    "                    avg_validation_accuracy.append(accuracy_vali.item())\n",
    "                    \n",
    "                    total_vali_loss_1 = np.array(total_vali_loss)\n",
    "                    np.save('total_vali_loss', total_vali_loss_1)      \n",
    "                    avg_validation_accuracy_1 = np.array(avg_validation_accuracy)\n",
    "                    np.save('avg_validation_accuracy', avg_validation_accuracy_1)      \n",
    "                \n",
    "                    if total_vali_loss[i] > best_loss and i != 0:\n",
    "                        early_stop += 1\n",
    "                        if early_stop == tolerence:\n",
    "                            print('early stop here')\n",
    "                            break\n",
    "                    else:\n",
    "                        best_loss = total_vali_loss[i] \n",
    "                        early_stop = 0\n",
    "                    i = i + 1\n",
    "            print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    print(\"Training complete after\", epoch, \"epochs\")\n",
    "    \n",
    "    print(\"total_vali_loss\")\n",
    "    print(total_vali_loss)\n",
    "    print(\"avg_minibatch_loss\")\n",
    "    print(avg_minibatch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s '.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=0, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "Epoch 1, average minibatch 1 loss: 11.953\n",
      "accuracy 0.0\n",
      "1 minibatch_time 2.0727810859680176\n",
      "validatation accuracy 11.25\n",
      "validation time =  393.77547955513\n",
      "tensor(5.9683, device='cuda:0')\n",
      "Epoch 1, average minibatch 2 loss: 5.937\n",
      "accuracy 0.03125\n",
      "1 minibatch_time 394.89313316345215\n",
      "validatation accuracy 17.291666666666668\n",
      "validation time =  394.7303397655487\n",
      "tensor(5.9346, device='cuda:0')\n",
      "Epoch 1, average minibatch 3 loss: 5.985\n",
      "accuracy 0.03125\n",
      "1 minibatch_time 395.86189675331116\n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=5,resume=False,direct='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display # to display images\n",
    "# for minibatch_count, (inputs, labels) in enumerate(train_loader, 0):\n",
    "#     print(inputs.shape)\n",
    "#     #inputs = inputs.to(computing_device)\n",
    "#     #labels = labels.to(computing_device)\n",
    "#     image = inputs.cpu().detach().numpy()[1].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[0].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     image = inputs.cpu().detach().numpy()[2].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "    \n",
    "#     image = inputs.cpu().detach().numpy()[3].reshape((224,224))*255\n",
    "    \n",
    "#     print(image.sum())\n",
    "#     plt.imshow(image.astype(int),)\n",
    "#     plt.show()\n",
    "#     #image = Image.fromarray(image.astype(int),\"L\")\n",
    "#     #display(image)\n",
    "#     if minibatch_count >10: \n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> loss = nn.CrossEntropyLoss()\n",
    "# >>> input = torch.randn(3, 5, requires_grad=True)\n",
    "# >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# >>> output = loss(input, target)\n",
    "# >>> output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
